<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://andreaneenu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://andreaneenu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-08T08:15:06+00:00</updated><id>https://andreaneenu.github.io/feed.xml</id><title type="html">blank</title><subtitle>Helooo, I&apos;m Andrea :) Welcome to my page </subtitle><entry><title type="html">Deep-ML: GPT-2 Text Generation</title><link href="https://andreaneenu.github.io/blog/2026/deepml-gpt2-text-generation/" rel="alternate" type="text/html" title="Deep-ML: GPT-2 Text Generation"/><published>2026-02-07T12:44:21+00:00</published><updated>2026-02-07T12:44:21+00:00</updated><id>https://andreaneenu.github.io/blog/2026/deepml-gpt2-text-generation</id><content type="html" xml:base="https://andreaneenu.github.io/blog/2026/deepml-gpt2-text-generation/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>I recently tackled the <strong>GPT-2 Text Generation</strong> problem on <a href="https://www.deep-ml.com/problems/88?from=Deep%20Learning">deep-ml.ai</a>, a hard-level challenge focusing on implementing the core components of transformer-based language models. This problem required building text generation from scratch, giving me deep insights into how GPT-2 actually works under the hood.</p> <h2 id="the-problem">The Problem</h2> <p><strong>Difficulty</strong>: Hard<br/> <strong>Link</strong>: <a href="https://www.deep-ml.com/problems/88?from=Deep%20Learning">https://www.deep-ml.com/problems/88</a></p> <p>The challenge was to implement GPT-2’s text generation mechanism, including:</p> <ul> <li>Token and positional embeddings</li> <li>Layer normalization</li> <li>Autoregressive text generation</li> <li>Logit calculation and token selection</li> </ul> <p>This problem strips away the high-level abstractions and forces you to understand the fundamental operations that make GPT-2 work.</p> <h2 id="my-approach">My Approach</h2> <p>My implementation focused on building the core pipeline step by step:</p> <h3 id="1-embedding-layer">1. <strong>Embedding Layer</strong></h3> <p>I combined word token embeddings (<code class="language-plaintext highlighter-rouge">wte</code>) with positional encodings (<code class="language-plaintext highlighter-rouge">wpe</code>) to give each token both semantic meaning and positional context:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">wte</span><span class="sh">"</span><span class="p">][</span><span class="n">tokens</span><span class="p">]</span> <span class="o">+</span> <span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">wpe</span><span class="sh">"</span><span class="p">][:</span><span class="n">seq_len</span><span class="p">]</span>
</code></pre></div></div> <p>This is crucial because transformers don’t inherently understand token order - positional encodings provide that information.</p> <h3 id="2-layer-normalization">2. <strong>Layer Normalization</strong></h3> <p>One of the trickiest parts was implementing layer normalization correctly. The key insight is that normalization must be applied <strong>per token</strong> (per row), not across the entire matrix:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">g</span><span class="o">*</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">epsilon</span><span class="p">))</span><span class="o">+</span><span class="n">b</span>
</code></pre></div></div> <p>Applying it incorrectly would mix statistics across different tokens, breaking the model’s behavior.</p> <h3 id="3-autoregressive-generation">3. <strong>Autoregressive Generation</strong></h3> <p>The generation loop iteratively produces one token at a time, using all previous tokens as context:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_tokens_to_generate</span><span class="p">):</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">wte</span><span class="sh">"</span><span class="p">][</span><span class="n">tokens</span><span class="p">]</span> <span class="o">+</span> <span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">wpe</span><span class="sh">"</span><span class="p">][:</span><span class="n">seq_len</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nf">layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">ln_f</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">g</span><span class="sh">"</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">ln_f</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">b</span><span class="sh">"</span><span class="p">])</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">wte</span><span class="sh">"</span><span class="p">].</span><span class="n">T</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>
</code></pre></div></div> <h3 id="4-greedy-decoding">4. <strong>Greedy Decoding</strong></h3> <p>I used greedy decoding (selecting the highest probability token) for simplicity and determinism:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">next_token</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></div> <h2 id="what-i-learned">What I Learned</h2> <p>This problem taught me several fundamental concepts:</p> <ul> <li><strong>Embeddings are additive</strong>: Token and positional embeddings are simply added together, which works because the model learns to use different dimensions for different purposes</li> <li><strong>Layer normalization mechanics</strong>: Understanding the exact mathematical operations and why they’re applied per token</li> <li><strong>Autoregressive generation</strong>: How language models generate text one token at a time, with each prediction depending on all previous tokens</li> <li><strong>The simplicity of the core loop</strong>: Despite GPT-2’s complexity, the basic generation loop is surprisingly straightforward</li> </ul> <h2 id="cool-aspects">Cool Aspects</h2> <p>What I found most fascinating:</p> <ol> <li> <p><strong>Layer normalization’s importance</strong>: Such a simple operation (normalize, scale, shift) has such a profound impact on training stability and model performance</p> </li> <li> <p><strong>Greedy vs. sampling trade-offs</strong>: While I used greedy decoding, understanding that temperature sampling and top-k/top-p sampling can dramatically change output quality and diversity</p> </li> <li> <p><strong>Building intuition</strong>: Implementing from scratch gave me a much deeper understanding than just using <code class="language-plaintext highlighter-rouge">transformers.pipeline()</code></p> </li> </ol> <h2 id="challenges--solutions">Challenges &amp; Solutions</h2> <p><strong>Challenge</strong>: Layer normalization axis confusion<br/> <strong>Solution</strong>: Carefully reviewed the mathematical definition and ensured normalization was applied per row (per token). Testing with known inputs helped verify correctness.</p> <p><strong>Challenge</strong>: Managing token indices correctly<br/> <strong>Solution</strong>: Tracked the initial prompt length separately to ensure only generated tokens were returned, not the entire sequence including the prompt.</p> <p><strong>Challenge</strong>: Understanding the flow of data<br/> <strong>Solution</strong>: Watched <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=1193s">Andrej Karpathy’s excellent GPT video</a> which provided invaluable intuition about how transformers process information.</p> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li> <p><strong>Implementation builds intuition</strong>: Writing GPT-2 from scratch gave me insights no amount of reading could provide</p> </li> <li> <p><strong>Details matter</strong>: Small mistakes like normalizing across the wrong axis can completely break the model</p> </li> <li> <p><strong>Simplicity at the core</strong>: Despite the hype around transformers, the fundamental operations are matrix multiplications, additions, and normalizations</p> </li> <li> <p><strong>Testing is crucial</strong>: Setting random seeds and testing with known inputs is essential for debugging generative models</p> </li> </ol> <h2 id="related-resources">Related Resources</h2> <ul> <li><a href="https://www.deep-ml.com/problems/88?from=Deep%20Learning">Problem on deep-ml.ai</a></li> <li><a href="https://github.com/andreaneenu/deep-ml_work">My solution on GitHub</a></li> <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=1193s">Andrej Karpathy’s GPT Video</a> - Highly recommended!</li> <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> - Original Transformer paper</li> <li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2 Paper</a></li> </ul> <hr/> <p><strong>Tags</strong>: deep-ml machine-learning gpt2 transformers nlp deep-learning text-generation language-models</p>]]></content><author><name></name></author><category term="deep-ml-journey"/><category term="deep-ml"/><category term="machine-learning"/><category term="gpt2"/><category term="transformers"/><category term="nlp"/><category term="deep-learning"/><category term="text-generation"/><category term="language-models"/><summary type="html"><![CDATA[Building GPT-2 text generation from scratch - understanding transformers, embeddings, and layer normalization]]></summary></entry></feed>